cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
n <- 128    # Number of observations per sample
lambda <- 0.1  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 32    # Number of observations per sample
lambda <- 0.5  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 64    # Number of observations per sample
lambda <- 0.5  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 128    # Number of observations per sample
lambda <- 0.5  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 32    # Number of observations per sample
lambda <- 1  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 64    # Number of observations per sample
lambda <- 1  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 128    # Number of observations per sample
lambda <- 1  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 128    # Number of observations per sample
lambda <- 0.1  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
summary(Xbars)
n <- 128    # Number of observations per sample
lambda <- 0.1  # Rate parameter for the exponential distribution
# Generate random samples
XSamples <- replicate(M, rexp(n, rate=lambda))
dim(XSamples)
# b)	Calculate the sample mean of each sample
Xbars <- colSums(XSamples) / n
# c) Obtain sampling distribution sample mean
hist(Xbars, nclass=30, freq=FALSE, main="Sampling Distribution of Xbar when n=16")
# d) Adding approximate density curve to the histogram above
dens <- density(Xbars)
lines(dens$x, dens$y, col=2)
# e) Getting some summary statistics about distribution
# Calculate and display summary statistics
mean_Xbars <- mean(Xbars)
var_Xbars <- var(Xbars)
sd_Xbars <- sd(Xbars)
# Display the results
cat("Mean of Xbars:", mean_Xbars, "\n")
cat("Variance of Xbars:", var_Xbars, "\n")
cat("Standard Deviation of Xbars:", sd_Xbars, "\n")
# B. Sampling from Normal Distribution
# a) Generate random samples and calculate the sample means
# Define the parameters
n <- 16    # Number of observations per sample
means <- c(10, 10, 20, 20)  # Mean values to use
sds <- c(1, 3, 1, 3)        # Standard deviation values to use
# Loop over each combination of mean and sd
for (i in 1:4) {
for (n in sample_sizes) {
# Generate samples from the normal distribution
XSamples <- replicate(M, rnorm(n, mean=means[i], sd=sds[i]))
Xbars <- colSums(XSamples) / n
# Plotting and summary statistics calculation skipped for brevity
cat("Mean:", means[i], "SD:", sds[i], "Sample size:", n, "\n")
cat("Mean of Xbars:", mean(Xbars), "\n")
cat("Standard Deviation of Xbars:", sd(Xbars), "\n\n")
}
}
# B. Sampling from Normal Distribution
# Define the parameters
sample_sizes <- c(16, 25, 36)
means <- c(10, 10, 20, 20)  # Mean values to use
sds <- c(1, 3, 1, 3)        # Standard deviation values to use
# Loop over each combination of mean and sd
for (i in 1:4) {
for (n in sample_sizes) {
# Generate samples from the normal distribution
XSamples <- replicate(M, rnorm(n, mean=means[i], sd=sds[i]))
Xbars <- colSums(XSamples) / n
# Plotting and summary statistics calculation skipped for brevity
cat("Mean:", means[i], "SD:", sds[i], "Sample size:", n, "\n")
cat("Mean of Xbars:", mean(Xbars), "\n")
cat("Standard Deviation of Xbars:", sd(Xbars), "\n\n")
}
}
# Loop over each combination of mean and sd
for (i in 1:4) {
for (n in sample_sizes) {
# Generate samples from the normal distribution
XSamples <- replicate(M, rnorm(n, mean=means[i], sd=sds[i]))
Xbars <- colSums(XSamples) / n
# Plotting and summary statistics calculation skipped for brevity
cat("Mean:", means[i], "SD:", sds[i], "Sample size:", n, "\n")
cat("Mean of Xbars:", mean(Xbars), "\n")
cat("Standard Deviation of Xbars:", sd(Xbars), "\n\n")
}
}
# A. Simulate random sample of 50 observations from this distribution
n <- 50
p_success <- 0.2
XSample <- rbinom(n, 1, p_success)
# B. Calculate the sample proportion of successes
p_hat_obs <- mean(XSample)
cat("Observed sample proportion, p̂_obs:", p_hat_obs, "\n")
# C. Evaluating observed proportion and checking if it is within 2 s.d
# Calculate the standard deviation of the sampling distribution of p̂
std_error <- sqrt((p_success * (1 - p_success)) / n)
# Check if p̂_obs is within 2 standard deviations of p_0
lower_bound <- p_success - 2 * std_error
upper_bound <- p_success + 2 * std_error
cat("Lower bound:", lower_bound, "\n")
cat("Upper bound:", upper_bound, "\n")
# Determine if p̂_obs supports p_0 = 0.2
if (p_hat_obs >= lower_bound && p_hat_obs <= upper_bound) {
cat("The observed p̂_obs supports the value of p being 0.2.\n")
} else {
cat("The observed p̂_obs does not support the value of p being 0.2.\n")
}
# ----------------------------------------------
# ** Task 3 **
# ----------------------------------------------
# A. Compare Pobs with P_0 = 0.4
# New hypothesized probability of success
p_new_success <- 0.4
# Calculate the new standard deviation of the sampling distribution of p̂
std_error_new <- sqrt((p_new_success * (1 - p_new_success)) / n)
# Check if p̂_obs is within 2 standard deviations of the new p_0
lower_bound_new <- p_new_success - 2 * std_error_new
upper_bound_new <- p_new_success + 2 * std_error_new
cat("New lower bound:", lower_bound_new, "\n")
cat("New upper bound:", upper_bound_new, "\n")
# Determine if p̂_obs is reasonably close to the new hypothesized value p_0 = 0.4
if (p_hat_obs >= lower_bound_new && p_hat_obs <= upper_bound_new) {
cat("The observed p̂_obs might be considered reasonably close to p_0 = 0.4.\n")
} else {
cat("The observed p̂_obs is not reasonably close to p_0 = 0.4.\n")
}
# B. How typical or extreme p_obs is
# Simulate 10,000 random samples of size 50 each from Bernoulli distribution with p = 0.4
n <- 50
XSamples_new <- replicate(10000, rbinom(n, 1, p_new_success))
# b) Calculate the sample proportion for each sample
hatpk <- colSums(XSamples_new) / n
# c) Plot the histogram of these sample proportions
hist(hatpk, xlab=expression(bar(X)[n]), main="Sampling Distribution under p_0 = 0.4", prob=TRUE)
# d) Calculate the p-value: the proportion of simulated proportions ≤ observed proportion
p_value <- mean(hatpk <= p_hat_obs) # p̂_obs: 0.28
cat("P-value:", p_value, "\n")
install.packages("arules")
install.packages("foreign")
library(arules)
library(foreign)
# Load the ARFF file
data <- read.arff("diabetes.arff")
# Load the ARFF file
data <- read.arff("./diabetes.arff")
# Load the ARFF file
data <- read.arff("diabetes.arff")
setwd("~/Desktop/CSCI-317_Computation-for-Scientific-Applications_Project-And-Labs/Lab7_AssociationRuleAnalysis")
# Load the ARFF file
data <- read.arff("diabetes.arff")
# View the first few rows of the dataset
head(data)
# We need to convert numeric attributes to categorical for association rule mining
data$preg <- cut(data$preg, breaks=c(-Inf, 0, 3, 6, Inf), labels=c("None", "Low", "Medium", "High"))
data$plas <- cut(data$plas, breaks=quantile(data$plas, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pres <- cut(data$pres, breaks=quantile(data$pres, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$skin <- cut(data$skin, breaks=quantile(data$skin, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$insu <- cut(data$insu, breaks=quantile(data$insu, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$mass <- cut(data$mass, breaks=quantile(data$mass, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pedi <- cut(data$pedi, breaks=quantile(data$pedi, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$age  <- cut(data$age, breaks=c(-Inf, 25, 50, Inf), labels=c("Young", "Middle-Aged", "Senior"))
# Converting the class attribute to a factor for association analysis
data$class <- as.factor(data$class)
# Mining association rules
rules <- apriori(data, parameter=list(supp=0.01, conf=0.8, target="rules"))
# Sorting the rules by confidence and lift
rules.sorted <- sort(rules, by="lift")
# Inspecting the top 10 rules
inspect(rules.sorted[1:10])
# Save the rules to a file
write(rules, file="diabetes_rules.txt")
# View the dataset and rules for validation
head(data)
inspect(rules.sorted[1:10])
# Make sure to set your own working directory
setwd("~/Desktop/CSCI-317_Computation-for-Scientific-Applications_Project-And-Labs/Lab7_AssociationRuleAnalysis")
# Installing required packages
install.packages("arules")
install.packages("arules")
install.packages("foreign")
install.packages("foreign")
library(arules)
library(foreign)
# Load the ARFF file
data <- read.arff("diabetes.arff")
# Viewing first 5 rows of the dataset
head(data)
# We need to convert numeric attributes to categorical for association rule mining
data$preg <- cut(data$preg, breaks=c(-Inf, 0, 3, 6, Inf), labels=c("None", "Low", "Medium", "High"))
data$plas <- cut(data$plas, breaks=quantile(data$plas, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pres <- cut(data$pres, breaks=quantile(data$pres, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$skin <- ifelse(data$skin == 0, "Not Measured", ifelse(data$skin < 20, "Low", ifelse(data$skin < 35, "Medium", "High")))
data$insu <- ifelse(data$insu == 0, "Not Measured", ifelse(data$insu < 94, "Low", ifelse(data$insu < 168, "Medium", "High")))
data$mass <- cut(data$mass, breaks=quantile(data$mass, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pedi <- cut(data$pedi, breaks=quantile(data$pedi, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$age  <- cut(data$age, breaks=c(-Inf, 25, 50, Inf), labels=c("Young", "Middle-Aged", "Senior"))
# Converting the class attribute to a factor for association analysis
data$class <- as.factor(data$class)
# Ensuring all variables are factors or logicals
data[] <- lapply(data, function(x) if (is.numeric(x)) factor(x) else x)
# Mining association rules
rules <- apriori(data, parameter=list(supp=0.01, conf=0.8, target="rules"))
# Make sure to set your own working directory
setwd("~/Desktop/CSCI-317_Computation-for-Scientific-Applications_Project-And-Labs/Lab7_AssociationRuleAnalysis")
library(arules)
library(foreign)
# Load the ARFF file
data <- read.arff("diabetes.arff")
# Viewing first 5 rows of the dataset
head(data)
# We need to convert numeric attributes to categorical for association rule mining
data$preg <- cut(data$preg, breaks=c(-Inf, 0, 3, 6, Inf), labels=c("None", "Low", "Medium", "High"))
data$plas <- cut(data$plas, breaks=quantile(data$plas, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pres <- cut(data$pres, breaks=quantile(data$pres, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$skin <- factor(ifelse(data$skin == 0, "Not Measured", ifelse(data$skin < 20, "Low", ifelse(data$skin < 35, "Medium", "High"))))
data$insu <- factor(ifelse(data$insu == 0, "Not Measured", ifelse(data$insu < 94, "Low", ifelse(data$insu < 168, "Medium", "High"))))
data$mass <- cut(data$mass, breaks=quantile(data$mass, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pedi <- cut(data$pedi, breaks=quantile(data$pedi, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$age  <- cut(data$age, breaks=c(-Inf, 25, 50, Inf), labels=c("Young", "Middle-Aged", "Senior"))
# Ensure all variables are explicitly converted to factors
data[] <- lapply(data, function(x) if(is.numeric(x)) factor(x) else x)
# Mining association rules
rules <- apriori(data, parameter=list(supp=0.01, conf=0.8, target="rules"))
# Sorting the rules by confidence and lift
rules.sorted <- sort(rules, by="lift")
# Inspecting the top 10 rules
inspect(rules.sorted[1:10])
# Save the rules to a file
write(rules, file="diabetes_rules.txt")
# View the dataset and rules for validation
head(data)
inspect(rules.sorted[1:10])
# Make sure to set your own working directory
setwd("~/Desktop/CSCI-317_Computation-for-Scientific-Applications_Project-And-Labs/Lab7_AssociationRuleAnalysis")
library(arules)
library(foreign)
# Load the ARFF file
data <- read.arff("diabetes.arff")
# Viewing first 5 rows of the dataset
head(data)
# We need to convert numeric attributes to categorical for association rule mining
data$preg <- cut(data$preg, breaks=c(-Inf, 0, 3, 6, Inf), labels=c("None", "Low", "Medium", "High"))
data$plas <- cut(data$plas, breaks=quantile(data$plas, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pres <- cut(data$pres, breaks=quantile(data$pres, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$skin <- factor(ifelse(data$skin == 0, "Not Measured", ifelse(data$skin < 20, "Low", ifelse(data$skin < 35, "Medium", "High"))))
data$insu <- factor(ifelse(data$insu == 0, "Not Measured", ifelse(data$insu < 94, "Low", ifelse(data$insu < 168, "Medium", "High"))))
data$mass <- cut(data$mass, breaks=quantile(data$mass, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pedi <- cut(data$pedi, breaks=quantile(data$pedi, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$age  <- cut(data$age, breaks=c(-Inf, 25, 50, Inf), labels=c("Young", "Middle-Aged", "Senior"))
# Ensure all variables are explicitly converted to factors
data[] <- lapply(data, function(x) if(is.numeric(x)) factor(x) else x)
# Assuming 'data' is your transactions dataset
rules <- apriori(data, parameter = list(supp = 0.1, conf = 0.9, target = "rules", maxlen = 10))
rules.sorted <- sort(rules, by = "confidence")
# Inspecting the top 10 rules
inspect(head(rules.sorted, n = 10))
# Save the rules to a file
write(rules, file="diabetes_rules.txt")
# View the dataset and rules for validation
head(data)
inspect(rules.sorted[1:10])
# Make sure to set your own working directory
setwd("~/Desktop/CSCI-317_Computation-for-Scientific-Applications_Project-And-Labs/Lab7_AssociationRuleAnalysis")
library(arules)
library(foreign)
# Load the ARFF file
data <- read.arff("diabetes.arff")
# Viewing first 5 rows of the dataset
head(data)
# We need to convert numeric attributes to categorical for association rule mining
data$preg <- cut(data$preg, breaks=c(-Inf, 0, 3, 6, Inf), labels=c("None", "Low", "Medium", "High"))
data$plas <- cut(data$plas, breaks=quantile(data$plas, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pres <- cut(data$pres, breaks=quantile(data$pres, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$skin <- factor(
ifelse(data$skin == 0, "Not Measured",
ifelse(data$skin < 20, "Low",
ifelse(data$skin < 35, "Medium", "High")
)
)
)
data$insu <- factor(
ifelse(data$insu == 0, "Not Measured",
ifelse(data$insu < 94, "Low",
ifelse(data$insu < 168, "Medium", "High")
)
)
)
data$mass <- cut(data$mass, breaks=quantile(data$mass, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pedi <- cut(data$pedi, breaks=quantile(data$pedi, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$age  <- cut(data$age, breaks=c(-Inf, 25, 50, Inf), labels=c("Young", "Middle-Aged", "Senior"))
# Ensure all variables are explicitly converted to factors
data[] <- lapply(data, function(x) if(is.numeric(x)) factor(x) else x)
# Assuming 'data' is your transactions dataset
rules <- apriori(data, parameter = list(supp = 0.1, conf = 0.9, target = "rules", maxlen = 10))
rules.sorted <- sort(rules, by = "confidence")
# Inspecting the top 10 rules
inspect(head(rules.sorted, n = 10))
# Save the rules to a file
write(rules, file="diabetes_rules.txt")
# View the dataset and rules for validation
head(data)
inspect(rules.sorted[1:10])
# Make sure to set your own working directory
setwd("~/Desktop/CSCI-317_Computation-for-Scientific-Applications_Project-And-Labs/Lab7_AssociationRuleAnalysis")
library(arules)
library(foreign)
# Load the ARFF file
data <- read.arff("diabetes.arff")
# Viewing first 5 rows of the dataset to make sure it was loaded properly
head(data)
# We need to convert numeric attributes to categorical for association rule mining
# Since I don't have specific domain knowledge, the ranges/values/thresholds might be inaccurate but are a good starting point
data$preg <- cut(data$preg, breaks=c(-Inf, 0, 3, 6, Inf), labels=c("None", "Low", "Medium", "High"))
data$plas <- cut(data$plas, breaks=quantile(data$plas, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pres <- cut(data$pres, breaks=quantile(data$pres, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$skin <- factor(
ifelse(data$skin == 0, "Not Measured",
ifelse(data$skin < 20, "Low",
ifelse(data$skin < 35, "Medium", "High")
)
)
)
data$insu <- factor(
ifelse(data$insu == 0, "Not Measured",
ifelse(data$insu < 94, "Low",
ifelse(data$insu < 168, "Medium", "High")
)
)
)
data$mass <- cut(data$mass, breaks=quantile(data$mass, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$pedi <- cut(data$pedi, breaks=quantile(data$pedi, na.rm=TRUE), include.lowest=TRUE, labels=c("Low", "Medium-Low", "Medium-High", "High"))
data$age  <- cut(data$age, breaks=c(-Inf, 25, 50, Inf), labels=c("Young", "Middle-Aged", "Senior"))
# Ensure all variables are converted to factors to avoid warnings
# Convert numeric columns to factors
convert_numeric_to_factor <- function(x) {
if (is.numeric(x)) {
x <- factor(x)
}
x
}
data[] <- lapply(data, convert_numeric_to_factor)
# Association rule analysis using apriori
# The thresholds/settings were set similar to the one used in Weka
rules <- apriori(data, parameter = list(supp = 0.1, conf = 0.9, target = "rules", maxlen = 10))
rules.sorted <- sort(rules, by = "confidence")
# Inspecting the top 10 rules
inspect(head(rules.sorted, n = 10))
# Save the rules to a file
write(rules, file="diabetes_rules.txt")
# View the dataset and rules for validation
head(data)
inspect(rules.sorted[1:10])
